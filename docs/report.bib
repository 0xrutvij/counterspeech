




% Zhang, Yizhe et al. “DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation.” ArXiv abs/1911.00536 (2019): n. pag.


@article{williams2019hatred,
  title     = {Hatred behind the screens: A report on the rise of online hate speech},
  author    = {Williams, Matthew},
  year      = {2019},
  publisher = {Mishcon de Reya}
}

@inproceedings{chung-etal-2019-conan,
  title     = {{CONAN} - {CO}unter {NA}rratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech},
  author    = {Chung, Yi-Ling  and
               Kuzmenko, Elizaveta  and
               Tekiroglu, Serra Sinem  and
               Guerini, Marco},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P19-1271},
  doi       = {10.18653/v1/P19-1271},
  pages     = {2819--2829},
  abstract  = {Although there is an unprecedented effort to provide adequate responses in terms of laws and policies to hate content on social media platforms, dealing with hatred online is still a tough problem. Tackling hate speech in the standard way of content deletion or user suspension may be charged with censorship and overblocking. One alternate strategy, that has received little attention so far by the research community, is to actually oppose hate content with counter-narratives (i.e. informed textual responses). In this paper, we describe the creation of the first large-scale, multilingual, expert-based dataset of hate-speech/counter-narrative pairs. This dataset has been built with the effort of more than 100 operators from three different NGOs that applied their training and expertise to the task. Together with the collected data we also provide additional annotations about expert demographics, hate and response type, and data augmentation through translation and paraphrasing. Finally, we provide initial experiments to assess the quality of our data.}
}

@article{nockleyby2000hate,
  title   = {‘Hate speech in Encyclopedia of the American Constitution},
  author  = {Nockleyby, J},
  journal = {Electronic Journal of Academic and Special Librarianship},
  year    = {2000}
}

@misc{towardsdatascienceDialoGPTLargeScale,
  author       = {Prakhar Mishra},
  title        = {{D}ialo{G}{P}{T}: {L}arge-{S}cale {G}enerative {P}re-training for {C}onversational {R}esponse {G}eneration --- towardsdatascience.com},
  howpublished = {\url{https://towardsdatascience.com/dialogpt-large-scale-generative-pre-training-for-conversational-response-generation-5ceb783428dc}},
  year         = {},
  note         = {[Accessed 08-May-2023]}
}

@misc{huggingfaceDialoGPT,
  author       = {HuggingFace},
  title        = {{D}ialo{G}{P}{T} --- huggingface.co},
  howpublished = {\url{https://huggingface.co/docs/transformers/model_doc/dialogpt}},
  year         = {},
  note         = {[Accessed 08-May-2023]}
}


@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}

@article{zhang2019dialogpt,
  title   = {Dialogpt: Large-scale generative pre-training for conversational response generation},
  author  = {Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill},
  journal = {arXiv preprint arXiv:1911.00536},
  year    = {2019}
}

@article{chen2021bert2bert,
  author     = {Cheng Chen and
                Yichun Yin and
                Lifeng Shang and
                Xin Jiang and
                Yujia Qin and
                Fengyu Wang and
                Zhi Wang and
                Xiao Chen and
                Zhiyuan Liu and
                Qun Liu},
  title      = {bert2BERT: Towards Reusable Pretrained Language Models},
  journal    = {CoRR},
  volume     = {abs/2110.07143},
  year       = {2021},
  url        = {https://arxiv.org/abs/2110.07143},
  eprinttype = {arXiv},
  eprint     = {2110.07143},
  timestamp  = {Thu, 04 May 2023 17:10:57 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2110-07143.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{papineni2002bleu,
  title     = {Bleu: a method for automatic evaluation of machine translation},
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages     = {311--318},
  year      = {2002}
}

@article{gehman2020realtoxicityprompts,
  title   = {Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author  = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal = {arXiv preprint arXiv:2009.11462},
  year    = {2020}
}
